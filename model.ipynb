{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import dask.dataframe as dd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import Flatten, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import math\n",
    "import re\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>C0</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th class='vellipsis'>&hellip;</th><th>C1577903</th><th>C1577904</th><th>C1577905</th><th>C1577906</th><th>C1577907</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>3.83985e-22</td><td>7.69554e-22</td><td>1.13743e-21</td><td>1.44821e-21</td><td>1.6863e-21</td><td>1.83974e-21</td><td>1.90084e-21</td><td>1.86652e-21</td><td>1.73852e-21</td><td>1.52325e-21</td><td class=vellipsis>&hellip;</td><td>4.45936e-22</td><td>&minus;2.50303e-21</td><td>&minus;3.98991e-21</td><td>&minus;4.32521e-21</td><td>&minus;4.13826e-21</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>&minus;1.01416e-21</td><td>&minus;7.74709e-22</td><td>&minus;4.99028e-22</td><td>&minus;2.07651e-22</td><td>9.02605e-23</td><td>3.85339e-22</td><td>6.68307e-22</td><td>9.30265e-22</td><td>1.16298e-21</td><td>1.35912e-21</td><td class=vellipsis>&hellip;</td><td>&minus;4.13849e-21</td><td>&minus;3.49586e-21</td><td>&minus;2.28605e-21</td><td>&minus;2.24409e-21</td><td>&minus;3.91962e-21</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>1.20966e-21</td><td>1.13838e-21</td><td>1.04019e-21</td><td>9.20266e-22</td><td>7.81108e-22</td><td>6.25626e-22</td><td>4.5707e-22</td><td>2.7896e-22</td><td>9.50192e-23</td><td>&minus;9.09095e-23</td><td class=vellipsis>&hellip;</td><td>5.23381e-23</td><td>&minus;3.91688e-22</td><td>&minus;8.04927e-22</td><td>&minus;1.16927e-21</td><td>&minus;1.47168e-21</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>&minus;2.42755e-21</td><td>&minus;2.25382e-21</td><td>&minus;2.06635e-21</td><td>&minus;1.87457e-21</td><td>&minus;1.67887e-21</td><td>&minus;1.47966e-21</td><td>&minus;1.27737e-21</td><td>&minus;1.0724e-21</td><td>&minus;8.65188e-22</td><td>&minus;6.5617e-22</td><td class=vellipsis>&hellip;</td><td>&minus;1.21095e-21</td><td>&minus;1.4834e-21</td><td>&minus;1.75128e-21</td><td>&minus;2.01394e-21</td><td>&minus;2.27078e-21</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>&minus;6.66703e-22</td><td>&minus;6.96611e-22</td><td>&minus;7.26391e-22</td><td>&minus;7.5435e-22</td><td>&minus;7.80419e-22</td><td>&minus;8.04532e-22</td><td>&minus;8.26629e-22</td><td>&minus;8.46655e-22</td><td>&minus;8.6456e-22</td><td>&minus;8.80298e-22</td><td class=vellipsis>&hellip;</td><td>&minus;1.01421e-21</td><td>&minus;1.02737e-21</td><td>&minus;1.03796e-21</td><td>&minus;1.04596e-21</td><td>&minus;1.05137e-21</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>2.85836e-22</td><td>1.08388e-22</td><td>&minus;1.08196e-22</td><td>&minus;2.9329e-22</td><td>&minus;3.93027e-22</td><td>&minus;3.78379e-22</td><td>&minus;2.53611e-22</td><td>&minus;5.50333e-23</td><td>1.5956e-22</td><td>3.27715e-22</td><td class=vellipsis>&hellip;</td><td>&minus;6.98691e-22</td><td>1.11654e-21</td><td>1.07112e-21</td><td>9.94455e-22</td><td>1.08191e-21</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>&minus;1.23525e-21</td><td>&minus;1.05868e-21</td><td>&minus;8.7101e-22</td><td>&minus;6.81826e-22</td><td>&minus;4.91454e-22</td><td>&minus;3.00223e-22</td><td>&minus;1.08468e-22</td><td>8.34783e-23</td><td>2.75282e-22</td><td>4.66609e-22</td><td class=vellipsis>&hellip;</td><td>&minus;5.17145e-21</td><td>&minus;5.07792e-21</td><td>&minus;4.97446e-21</td><td>&minus;4.86139e-21</td><td>&minus;4.73902e-21</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>2.66315e-21</td><td>2.78778e-21</td><td>2.91335e-21</td><td>3.0329e-21</td><td>3.14618e-21</td><td>3.25295e-21</td><td>3.35299e-21</td><td>3.4461e-21</td><td>3.53209e-21</td><td>3.61077e-21</td><td class=vellipsis>&hellip;</td><td>8.35826e-23</td><td>2.85423e-22</td><td>4.86438e-22</td><td>6.86225e-22</td><td>8.84389e-22</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>8.56012e-21</td><td>8.36924e-21</td><td>8.14821e-21</td><td>7.90709e-21</td><td>7.64647e-21</td><td>7.36699e-21</td><td>7.06935e-21</td><td>6.75429e-21</td><td>6.42256e-21</td><td>6.07501e-21</td><td class=vellipsis>&hellip;</td><td>7.88476e-21</td><td>8.36311e-21</td><td>8.8129e-21</td><td>9.2331e-21</td><td>9.6228e-21</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>7.01216e-21</td><td>6.97063e-21</td><td>6.89357e-21</td><td>6.78271e-21</td><td>6.63858e-21</td><td>6.4619e-21</td><td>6.25352e-21</td><td>6.01446e-21</td><td>5.74591e-21</td><td>5.44918e-21</td><td class=vellipsis>&hellip;</td><td>2.02695e-21</td><td>2.85555e-21</td><td>3.65284e-21</td><td>4.41312e-21</td><td>5.13134e-21</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>2.29635e-21</td><td>1.93944e-21</td><td>1.42605e-21</td><td>8.08439e-22</td><td>1.31747e-22</td><td>&minus;5.5457e-22</td><td>&minus;1.20036e-21</td><td>&minus;1.75842e-21</td><td>&minus;2.18796e-21</td><td>&minus;2.45761e-21</td><td class=vellipsis>&hellip;</td><td>1.28064e-20</td><td>&minus;1.45171e-20</td><td>&minus;1.45491e-20</td><td>4.7175e-21</td><td>7.12564e-21</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.95052e-21</td><td>1.07803e-21</td><td>9.18412e-23</td><td>&minus;9.00359e-22</td><td>&minus;1.83363e-21</td><td>&minus;2.64687e-21</td><td>&minus;3.28687e-21</td><td>&minus;3.71173e-21</td><td>&minus;3.89364e-21</td><td>&minus;3.82071e-21</td><td class=vellipsis>&hellip;</td><td>1.73135e-20</td><td>&minus;1.74404e-20</td><td>&minus;1.43421e-20</td><td>1.85986e-20</td><td>&minus;1.88047e-20</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>&minus;5.59301e-21</td><td>&minus;5.17267e-21</td><td>&minus;4.62971e-21</td><td>&minus;3.99437e-21</td><td>&minus;3.27931e-21</td><td>&minus;2.49881e-21</td><td>&minus;1.66844e-21</td><td>&minus;8.0477e-22</td><td>7.49672e-23</td><td>9.53215e-22</td><td class=vellipsis>&hellip;</td><td>&minus;7.82387e-21</td><td>&minus;6.61698e-21</td><td>&minus;5.32265e-21</td><td>&minus;3.99734e-21</td><td>&minus;2.68864e-21</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>&minus;5.44469e-21</td><td>&minus;5.92893e-21</td><td>&minus;6.34981e-21</td><td>&minus;6.67325e-21</td><td>&minus;6.89426e-21</td><td>&minus;7.00947e-21</td><td>&minus;7.0171e-21</td><td>&minus;6.91704e-21</td><td>&minus;6.71081e-21</td><td>&minus;6.4016e-21</td><td class=vellipsis>&hellip;</td><td>1.11336e-20</td><td>1.14898e-20</td><td>1.1627e-20</td><td>1.15759e-20</td><td>1.13697e-20</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>1.49696e-21</td><td>1.19752e-21</td><td>8.10988e-22</td><td>3.7469e-22</td><td>&minus;8.46014e-23</td><td>&minus;5.38695e-22</td><td>&minus;9.59721e-22</td><td>&minus;1.32184e-21</td><td>&minus;1.60282e-21</td><td>&minus;1.78543e-21</td><td class=vellipsis>&hellip;</td><td>&minus;5.37003e-21</td><td>8.13023e-21</td><td>&minus;7.95295e-21</td><td>&minus;6.04547e-21</td><td>&minus;8.28666e-21</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>238</td><td>6.32591e-22</td><td>9.94579e-22</td><td>1.3654e-21</td><td>1.72193e-21</td><td>2.06042e-21</td><td>2.37735e-21</td><td>2.66938e-21</td><td>2.93346e-21</td><td>3.16683e-21</td><td>3.36704e-21</td><td class=vellipsis>&hellip;</td><td>&minus;5.84503e-21</td><td>&minus;5.97409e-21</td><td>&minus;5.86647e-21</td><td>&minus;5.52794e-21</td><td>&minus;4.97342e-21</td></tr>\n",
       "    <tr><td class='row_index'>239</td><td>&minus;7.00536e-21</td><td>&minus;6.77033e-21</td><td>&minus;6.41299e-21</td><td>&minus;5.94878e-21</td><td>&minus;5.38544e-21</td><td>&minus;4.73234e-21</td><td>&minus;4.00038e-21</td><td>&minus;3.20176e-21</td><td>&minus;2.34977e-21</td><td>&minus;1.45863e-21</td><td class=vellipsis>&hellip;</td><td>1.7588e-21</td><td>7.56682e-21</td><td>1.212e-20</td><td>1.47493e-20</td><td>1.51515e-20</td></tr>\n",
       "    <tr><td class='row_index'>240</td><td>&minus;2.82419e-21</td><td>&minus;2.77661e-21</td><td>&minus;2.64855e-21</td><td>&minus;2.44416e-21</td><td>&minus;2.16934e-21</td><td>&minus;1.832e-21</td><td>&minus;1.44187e-21</td><td>&minus;1.01019e-21</td><td>&minus;5.49394e-22</td><td>&minus;7.27672e-23</td><td class=vellipsis>&hellip;</td><td>&minus;8.52645e-21</td><td>&minus;5.61017e-21</td><td>8.3124e-22</td><td>5.68248e-21</td><td>7.11433e-21</td></tr>\n",
       "    <tr><td class='row_index'>241</td><td>&minus;4.47756e-21</td><td>&minus;4.3718e-21</td><td>&minus;4.21553e-21</td><td>&minus;4.0148e-21</td><td>&minus;3.77172e-21</td><td>&minus;3.48886e-21</td><td>&minus;3.16921e-21</td><td>&minus;2.81612e-21</td><td>&minus;2.43334e-21</td><td>&minus;2.02489e-21</td><td class=vellipsis>&hellip;</td><td>&minus;1.22676e-21</td><td>&minus;2.59445e-21</td><td>&minus;3.85683e-21</td><td>&minus;4.96471e-21</td><td>&minus;5.87556e-21</td></tr>\n",
       "    <tr><td class='row_index'>242</td><td>&minus;2.44675e-21</td><td>&minus;2.55246e-21</td><td>&minus;2.51868e-21</td><td>&minus;2.33818e-21</td><td>&minus;2.02147e-21</td><td>&minus;1.58701e-21</td><td>&minus;1.06009e-21</td><td>&minus;4.71411e-22</td><td>1.44725e-22</td><td>7.52432e-22</td><td class=vellipsis>&hellip;</td><td>6.94866e-21</td><td>8.64999e-22</td><td>&minus;6.09522e-21</td><td>&minus;6.5838e-21</td><td>&minus;2.73347e-21</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>243 rows &times; 1,577,908 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#18038e461c0 243x1577908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dt.fread(r\"\\\\wsl.localhost\\Ubuntu-22.04\\home\\imperious\\ML and CVAE projects\\GW from EMRI\\New_GW_signal\\Newdatasignal_one_1.txt\",sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for j in range(df.ncols):\n",
    "#    for i in range(df.nrows):\n",
    "#        val = df[i, j]\n",
    "#        if 'I' in val:\n",
    "#            print(i,j)\n",
    "#            print(df[i,j])\n",
    "#            complex_val = complex(val.replace('I', 'j'))\n",
    "#            print(complex_val)\n",
    "#            break\n",
    "#            df[i, j] = str(abs(complex_val))\n",
    "#        else:\n",
    "#            df[i, j] = str(val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.874275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.790708</td>\n",
       "      <td>0.596903</td>\n",
       "      <td>0.125664</td>\n",
       "      <td>3.141593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800000</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.841555</td>\n",
       "      <td>1.944446</td>\n",
       "      <td>1.162389</td>\n",
       "      <td>0.502655</td>\n",
       "      <td>1.319469</td>\n",
       "      <td>2.356194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.235850</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>1.53938</td>\n",
       "      <td>2.70177</td>\n",
       "      <td>1.162389</td>\n",
       "      <td>0.973894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9000000</td>\n",
       "      <td>86</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.286453</td>\n",
       "      <td>2.040121</td>\n",
       "      <td>2.356194</td>\n",
       "      <td>1.507964</td>\n",
       "      <td>2.261947</td>\n",
       "      <td>3.110177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.832572</td>\n",
       "      <td>-0.747715</td>\n",
       "      <td>2.293363</td>\n",
       "      <td>1.476549</td>\n",
       "      <td>1.193805</td>\n",
       "      <td>2.953097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>4000000</td>\n",
       "      <td>66</td>\n",
       "      <td>0.424</td>\n",
       "      <td>7.222384</td>\n",
       "      <td>2.995444</td>\n",
       "      <td>2.890265</td>\n",
       "      <td>2.953097</td>\n",
       "      <td>0.973894</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2000000</td>\n",
       "      <td>89</td>\n",
       "      <td>0.424</td>\n",
       "      <td>9.870412</td>\n",
       "      <td>0.110881</td>\n",
       "      <td>1.53938</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>0.345575</td>\n",
       "      <td>0.219911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>900000</td>\n",
       "      <td>89</td>\n",
       "      <td>0.424</td>\n",
       "      <td>14.051228</td>\n",
       "      <td>2.472069</td>\n",
       "      <td>2.513274</td>\n",
       "      <td>2.70177</td>\n",
       "      <td>1.507964</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>4000000</td>\n",
       "      <td>65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>7.204781</td>\n",
       "      <td>2.342268</td>\n",
       "      <td>2.136283</td>\n",
       "      <td>2.54469</td>\n",
       "      <td>2.858849</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>700000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.424</td>\n",
       "      <td>13.121000</td>\n",
       "      <td>1.944446</td>\n",
       "      <td>1.047198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          C0  C1     C2         C3        C4        C5        C6        C7  \\\n",
       "0     700000  40  0.000  13.874275  0.000000  1.790708  0.596903  0.125664   \n",
       "1     800000  72  0.000  14.841555  1.944446  1.162389  0.502655  1.319469   \n",
       "2    2000000  34  0.000   9.235850  0.003569   1.53938   2.70177  1.162389   \n",
       "3    9000000  86  0.000   7.286453  2.040121  2.356194  1.507964  2.261947   \n",
       "4    9000000  34  0.000   6.832572 -0.747715  2.293363  1.476549  1.193805   \n",
       "..       ...  ..    ...        ...       ...       ...       ...       ...   \n",
       "238  4000000  66  0.424   7.222384  2.995444  2.890265  2.953097  0.973894   \n",
       "239  2000000  89  0.424   9.870412  0.110881   1.53938  0.031416  0.345575   \n",
       "240   900000  89  0.424  14.051228  2.472069  2.513274   2.70177  1.507964   \n",
       "241  4000000  65  0.424   7.204781  2.342268  2.136283   2.54469  2.858849   \n",
       "242   700000  40  0.424  13.121000  1.944446  1.047198       0.0  0.392699   \n",
       "\n",
       "           C8  \n",
       "0    3.141593  \n",
       "1    2.356194  \n",
       "2    0.973894  \n",
       "3    3.110177  \n",
       "4    2.953097  \n",
       "..        ...  \n",
       "238  2.764602  \n",
       "239  0.219911  \n",
       "240  2.764602  \n",
       "241  2.764602  \n",
       "242       0.0  \n",
       "\n",
       "[243 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dt.fread(r\"\\\\wsl.localhost\\Ubuntu-22.04\\home\\imperious\\ML and CVAE projects\\GW from EMRI\\New_GW_signal\\paramlist1.txt\",sep=\"\\t\")\n",
    "params=params[:250,:]\n",
    "params = params.to_pandas()\n",
    "\n",
    "def calcular_formula(formula):\n",
    "  formula = formula.replace(\"Pi\", str(math.pi))\n",
    "  return eval(formula)\n",
    "\n",
    "for i in range(5,9):\n",
    "  params.iloc[:,i:i+1] = params.iloc[:,i:i+1].map(lambda x: calcular_formula(str(x)))\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_X = MinMaxScaler()\n",
    "normalizer_X.fit(params) #Fitted with continuous_x. Use it to decode MinMax.\n",
    "\n",
    "norm_params = normalizer_X.transform(params)\n",
    "\n",
    "#print(pd.DataFrame(norm_Params))\n",
    "\n",
    "#normalizing gw data\n",
    "normalizer_cond = MinMaxScaler()\n",
    "normalizer_cond.fit(df)\n",
    "\n",
    "df = normalizer_cond.transform(df) #does sending to the df itself save ram?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2\n",
    "batch_size = 32  #Why does 32 work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_and_batchnorm(x):\n",
    "    return Dropout(0.3)(BatchNormalization()(x))\n",
    "\n",
    "def noiser(args):\n",
    "    global mean, log_var\n",
    "    mean, log_var = args\n",
    "    N = K.random_normal(shape=(K.shape(mean)[0],hidden_dim), mean=0., stddev=1.0)\n",
    "    return K.exp(log_var / 2) * N + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_params = Input(shape=(9,))\n",
    "cond_gw = Input(shape=(1577908,))\n",
    "\n",
    "x = concatenate([input_params, cond_gw])\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = dropout_and_batchnorm(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = dropout_and_batchnorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space\n",
    "mean = Dense(hidden_dim)(x)\n",
    "log_var = Dense(hidden_dim)(x)\n",
    "h = Lambda(noiser, output_shape=(hidden_dim), name=\"latent_space\")([mean, log_var]) # MAIN ERROR. FIX THE SHAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "input_decoder = Input(shape=(hidden_dim,))\n",
    "cond_gw_decoder = Input(shape=(1577908,))\n",
    "d = concatenate([input_decoder,cond_gw_decoder])\n",
    "d = Dense(32, activation=\"relu\")(d)\n",
    "d = dropout_and_batchnorm(d)\n",
    "d = Dense(64, activation=\"relu\")(d)\n",
    "d = dropout_and_batchnorm(d)\n",
    "decoded = Dense(9, activation=\"linear\")(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, y):\n",
    "    loss = K.sum(K.square(x-y), axis=-1)\n",
    "    kl_loss = -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis=-1)\n",
    "    return loss + kl_loss\n",
    "\n",
    "def relative_precision(true,pred):\n",
    "    rp = K.sum(K.square(pred - true)/K.square(true))\n",
    "    return rp\n",
    "    #sqrt((weighted) sum (estimation - true value)^2/true value^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1577908)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 2)            100989348   ['input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 1577908)]    0           []                               \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 9)            50496233    ['encoder[0][0]',                \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 151,485,581\n",
      "Trainable params: 151,485,197\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.Model([input_params,cond_gw], h, name=\"encoder\") # Parece estar certo\n",
    "decoder = keras.Model([input_decoder, cond_gw_decoder], decoded, name=\"decoder\") \n",
    "\n",
    "cvae = keras.Model(inputs = [input_params, cond_gw, cond_gw_decoder], #its ok\n",
    "                   outputs = decoder([encoder([input_params, cond_gw]), cond_gw_decoder]),\n",
    "                   name=\"cvae\")\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "\n",
    "cvae.compile(optimizer=\"adam\", loss=vae_loss,metrics=['mean_absolute_error'])\n",
    "\n",
    "plot_model(encoder, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7/7 [==============================] - 15s 2s/step - loss: 11.3828 - mean_absolute_error: 0.7780 - val_loss: 2.3863 - val_mean_absolute_error: 0.4208\n",
      "Epoch 2/25\n",
      "7/7 [==============================] - 9s 1s/step - loss: 7.4979 - mean_absolute_error: 0.5536 - val_loss: 2.2081 - val_mean_absolute_error: 0.3988\n",
      "Epoch 3/25\n",
      "7/7 [==============================] - 9s 1s/step - loss: 5.3323 - mean_absolute_error: 0.4505 - val_loss: 2.0473 - val_mean_absolute_error: 0.3803\n",
      "Epoch 4/25\n",
      "7/7 [==============================] - 10s 2s/step - loss: 5.0305 - mean_absolute_error: 0.4028 - val_loss: 1.9008 - val_mean_absolute_error: 0.3641\n",
      "Epoch 5/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 5.6658 - mean_absolute_error: 0.4051 - val_loss: 1.7690 - val_mean_absolute_error: 0.3494\n",
      "Epoch 6/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 3.7853 - mean_absolute_error: 0.3609 - val_loss: 1.6480 - val_mean_absolute_error: 0.3371\n",
      "Epoch 7/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 3.7860 - mean_absolute_error: 0.3453 - val_loss: 1.5344 - val_mean_absolute_error: 0.3253\n",
      "Epoch 8/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 3.2747 - mean_absolute_error: 0.3247 - val_loss: 1.4298 - val_mean_absolute_error: 0.3150\n",
      "Epoch 9/25\n",
      "7/7 [==============================] - 9s 1s/step - loss: 3.2385 - mean_absolute_error: 0.3129 - val_loss: 1.3355 - val_mean_absolute_error: 0.3059\n",
      "Epoch 10/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.8543 - mean_absolute_error: 0.3062 - val_loss: 1.2518 - val_mean_absolute_error: 0.2975\n",
      "Epoch 11/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.7262 - mean_absolute_error: 0.2904 - val_loss: 1.1774 - val_mean_absolute_error: 0.2904\n",
      "Epoch 12/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.5175 - mean_absolute_error: 0.2838 - val_loss: 1.1103 - val_mean_absolute_error: 0.2839\n",
      "Epoch 13/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.2569 - mean_absolute_error: 0.2756 - val_loss: 1.0522 - val_mean_absolute_error: 0.2788\n",
      "Epoch 14/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.3363 - mean_absolute_error: 0.2720 - val_loss: 1.0023 - val_mean_absolute_error: 0.2743\n",
      "Epoch 15/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.1446 - mean_absolute_error: 0.2675 - val_loss: 0.9625 - val_mean_absolute_error: 0.2707\n",
      "Epoch 16/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.1608 - mean_absolute_error: 0.2661 - val_loss: 0.9301 - val_mean_absolute_error: 0.2675\n",
      "Epoch 17/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.0210 - mean_absolute_error: 0.2611 - val_loss: 0.9063 - val_mean_absolute_error: 0.2652\n",
      "Epoch 18/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.9088 - mean_absolute_error: 0.2580 - val_loss: 0.8873 - val_mean_absolute_error: 0.2633\n",
      "Epoch 19/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.9565 - mean_absolute_error: 0.2588 - val_loss: 0.8727 - val_mean_absolute_error: 0.2628\n",
      "Epoch 20/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.9019 - mean_absolute_error: 0.2579 - val_loss: 0.8616 - val_mean_absolute_error: 0.2628\n",
      "Epoch 21/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7980 - mean_absolute_error: 0.2584 - val_loss: 0.8540 - val_mean_absolute_error: 0.2627\n",
      "Epoch 22/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7579 - mean_absolute_error: 0.2587 - val_loss: 0.8497 - val_mean_absolute_error: 0.2627\n",
      "Epoch 23/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.6300 - mean_absolute_error: 0.2552 - val_loss: 0.8489 - val_mean_absolute_error: 0.2632\n",
      "Epoch 24/25\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.5006 - mean_absolute_error: 0.2596 - val_loss: 0.8462 - val_mean_absolute_error: 0.2633\n",
      "Epoch 25/25\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.5827 - mean_absolute_error: 0.2576 - val_loss: 0.8427 - val_mean_absolute_error: 0.2632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1803c52f090>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "cvae.fit(\n",
    "    [norm_params, df, df], norm_params,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    ") \n",
    "\n",
    "# Why is the loss NaN????   x.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

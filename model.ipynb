{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import dask.dataframe as dd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import Flatten, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>C0</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th class='vellipsis'>&hellip;</th><th>C1577903</th><th>C1577904</th><th>C1577905</th><th>C1577906</th><th>C1577907</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>3.83985e-22</td><td>7.69554e-22</td><td>1.13743e-21</td><td>1.44821e-21</td><td>1.6863e-21</td><td>1.83974e-21</td><td>1.90084e-21</td><td>1.86652e-21</td><td>1.73852e-21</td><td>1.52325e-21</td><td class=vellipsis>&hellip;</td><td>4.45936e-22</td><td>&minus;2.50303e-21</td><td>&minus;3.98991e-21</td><td>&minus;4.32521e-21</td><td>&minus;4.13826e-21</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>&minus;1.01416e-21</td><td>&minus;7.74709e-22</td><td>&minus;4.99028e-22</td><td>&minus;2.07651e-22</td><td>9.02605e-23</td><td>3.85339e-22</td><td>6.68307e-22</td><td>9.30265e-22</td><td>1.16298e-21</td><td>1.35912e-21</td><td class=vellipsis>&hellip;</td><td>&minus;4.13849e-21</td><td>&minus;3.49586e-21</td><td>&minus;2.28605e-21</td><td>&minus;2.24409e-21</td><td>&minus;3.91962e-21</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>1.20966e-21</td><td>1.13838e-21</td><td>1.04019e-21</td><td>9.20266e-22</td><td>7.81108e-22</td><td>6.25626e-22</td><td>4.5707e-22</td><td>2.7896e-22</td><td>9.50192e-23</td><td>&minus;9.09095e-23</td><td class=vellipsis>&hellip;</td><td>5.23381e-23</td><td>&minus;3.91688e-22</td><td>&minus;8.04927e-22</td><td>&minus;1.16927e-21</td><td>&minus;1.47168e-21</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>&minus;2.42755e-21</td><td>&minus;2.25382e-21</td><td>&minus;2.06635e-21</td><td>&minus;1.87457e-21</td><td>&minus;1.67887e-21</td><td>&minus;1.47966e-21</td><td>&minus;1.27737e-21</td><td>&minus;1.0724e-21</td><td>&minus;8.65188e-22</td><td>&minus;6.5617e-22</td><td class=vellipsis>&hellip;</td><td>&minus;1.21095e-21</td><td>&minus;1.4834e-21</td><td>&minus;1.75128e-21</td><td>&minus;2.01394e-21</td><td>&minus;2.27078e-21</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>&minus;6.66703e-22</td><td>&minus;6.96611e-22</td><td>&minus;7.26391e-22</td><td>&minus;7.5435e-22</td><td>&minus;7.80419e-22</td><td>&minus;8.04532e-22</td><td>&minus;8.26629e-22</td><td>&minus;8.46655e-22</td><td>&minus;8.6456e-22</td><td>&minus;8.80298e-22</td><td class=vellipsis>&hellip;</td><td>&minus;1.01421e-21</td><td>&minus;1.02737e-21</td><td>&minus;1.03796e-21</td><td>&minus;1.04596e-21</td><td>&minus;1.05137e-21</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>2.85836e-22</td><td>1.08388e-22</td><td>&minus;1.08196e-22</td><td>&minus;2.9329e-22</td><td>&minus;3.93027e-22</td><td>&minus;3.78379e-22</td><td>&minus;2.53611e-22</td><td>&minus;5.50333e-23</td><td>1.5956e-22</td><td>3.27715e-22</td><td class=vellipsis>&hellip;</td><td>&minus;6.98691e-22</td><td>1.11654e-21</td><td>1.07112e-21</td><td>9.94455e-22</td><td>1.08191e-21</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>&minus;1.23525e-21</td><td>&minus;1.05868e-21</td><td>&minus;8.7101e-22</td><td>&minus;6.81826e-22</td><td>&minus;4.91454e-22</td><td>&minus;3.00223e-22</td><td>&minus;1.08468e-22</td><td>8.34783e-23</td><td>2.75282e-22</td><td>4.66609e-22</td><td class=vellipsis>&hellip;</td><td>&minus;5.17145e-21</td><td>&minus;5.07792e-21</td><td>&minus;4.97446e-21</td><td>&minus;4.86139e-21</td><td>&minus;4.73902e-21</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>2.66315e-21</td><td>2.78778e-21</td><td>2.91335e-21</td><td>3.0329e-21</td><td>3.14618e-21</td><td>3.25295e-21</td><td>3.35299e-21</td><td>3.4461e-21</td><td>3.53209e-21</td><td>3.61077e-21</td><td class=vellipsis>&hellip;</td><td>8.35826e-23</td><td>2.85423e-22</td><td>4.86438e-22</td><td>6.86225e-22</td><td>8.84389e-22</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>8.56012e-21</td><td>8.36924e-21</td><td>8.14821e-21</td><td>7.90709e-21</td><td>7.64647e-21</td><td>7.36699e-21</td><td>7.06935e-21</td><td>6.75429e-21</td><td>6.42256e-21</td><td>6.07501e-21</td><td class=vellipsis>&hellip;</td><td>7.88476e-21</td><td>8.36311e-21</td><td>8.8129e-21</td><td>9.2331e-21</td><td>9.6228e-21</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>7.01216e-21</td><td>6.97063e-21</td><td>6.89357e-21</td><td>6.78271e-21</td><td>6.63858e-21</td><td>6.4619e-21</td><td>6.25352e-21</td><td>6.01446e-21</td><td>5.74591e-21</td><td>5.44918e-21</td><td class=vellipsis>&hellip;</td><td>2.02695e-21</td><td>2.85555e-21</td><td>3.65284e-21</td><td>4.41312e-21</td><td>5.13134e-21</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>2.29635e-21</td><td>1.93944e-21</td><td>1.42605e-21</td><td>8.08439e-22</td><td>1.31747e-22</td><td>&minus;5.5457e-22</td><td>&minus;1.20036e-21</td><td>&minus;1.75842e-21</td><td>&minus;2.18796e-21</td><td>&minus;2.45761e-21</td><td class=vellipsis>&hellip;</td><td>1.28064e-20</td><td>&minus;1.45171e-20</td><td>&minus;1.45491e-20</td><td>4.7175e-21</td><td>7.12564e-21</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.95052e-21</td><td>1.07803e-21</td><td>9.18412e-23</td><td>&minus;9.00359e-22</td><td>&minus;1.83363e-21</td><td>&minus;2.64687e-21</td><td>&minus;3.28687e-21</td><td>&minus;3.71173e-21</td><td>&minus;3.89364e-21</td><td>&minus;3.82071e-21</td><td class=vellipsis>&hellip;</td><td>1.73135e-20</td><td>&minus;1.74404e-20</td><td>&minus;1.43421e-20</td><td>1.85986e-20</td><td>&minus;1.88047e-20</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>&minus;5.59301e-21</td><td>&minus;5.17267e-21</td><td>&minus;4.62971e-21</td><td>&minus;3.99437e-21</td><td>&minus;3.27931e-21</td><td>&minus;2.49881e-21</td><td>&minus;1.66844e-21</td><td>&minus;8.0477e-22</td><td>7.49672e-23</td><td>9.53215e-22</td><td class=vellipsis>&hellip;</td><td>&minus;7.82387e-21</td><td>&minus;6.61698e-21</td><td>&minus;5.32265e-21</td><td>&minus;3.99734e-21</td><td>&minus;2.68864e-21</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>&minus;5.44469e-21</td><td>&minus;5.92893e-21</td><td>&minus;6.34981e-21</td><td>&minus;6.67325e-21</td><td>&minus;6.89426e-21</td><td>&minus;7.00947e-21</td><td>&minus;7.0171e-21</td><td>&minus;6.91704e-21</td><td>&minus;6.71081e-21</td><td>&minus;6.4016e-21</td><td class=vellipsis>&hellip;</td><td>1.11336e-20</td><td>1.14898e-20</td><td>1.1627e-20</td><td>1.15759e-20</td><td>1.13697e-20</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>&minus;1.66277e-22</td><td>2.34013e-22</td><td>6.01689e-22</td><td>8.31929e-22</td><td>8.7214e-22</td><td>7.13136e-22</td><td>3.91236e-22</td><td>&minus;2.00315e-23</td><td>&minus;4.26726e-22</td><td>&minus;7.35948e-22</td><td class=vellipsis>&hellip;</td><td>&minus;5.61256e-22</td><td>3.99178e-21</td><td>6.29461e-21</td><td>&minus;2.56849e-21</td><td><span class=na>NA</span></td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>245</td><td>6.32591e-22</td><td>9.94579e-22</td><td>1.3654e-21</td><td>1.72193e-21</td><td>2.06042e-21</td><td>2.37735e-21</td><td>2.66938e-21</td><td>2.93346e-21</td><td>3.16683e-21</td><td>3.36704e-21</td><td class=vellipsis>&hellip;</td><td>&minus;5.84503e-21</td><td>&minus;5.97409e-21</td><td>&minus;5.86647e-21</td><td>&minus;5.52794e-21</td><td>&minus;4.97342e-21</td></tr>\n",
       "    <tr><td class='row_index'>246</td><td>&minus;7.00536e-21</td><td>&minus;6.77033e-21</td><td>&minus;6.41299e-21</td><td>&minus;5.94878e-21</td><td>&minus;5.38544e-21</td><td>&minus;4.73234e-21</td><td>&minus;4.00038e-21</td><td>&minus;3.20176e-21</td><td>&minus;2.34977e-21</td><td>&minus;1.45863e-21</td><td class=vellipsis>&hellip;</td><td>1.7588e-21</td><td>7.56682e-21</td><td>1.212e-20</td><td>1.47493e-20</td><td>1.51515e-20</td></tr>\n",
       "    <tr><td class='row_index'>247</td><td>&minus;2.82419e-21</td><td>&minus;2.77661e-21</td><td>&minus;2.64855e-21</td><td>&minus;2.44416e-21</td><td>&minus;2.16934e-21</td><td>&minus;1.832e-21</td><td>&minus;1.44187e-21</td><td>&minus;1.01019e-21</td><td>&minus;5.49394e-22</td><td>&minus;7.27672e-23</td><td class=vellipsis>&hellip;</td><td>&minus;8.52645e-21</td><td>&minus;5.61017e-21</td><td>8.3124e-22</td><td>5.68248e-21</td><td>7.11433e-21</td></tr>\n",
       "    <tr><td class='row_index'>248</td><td>&minus;4.47756e-21</td><td>&minus;4.3718e-21</td><td>&minus;4.21553e-21</td><td>&minus;4.0148e-21</td><td>&minus;3.77172e-21</td><td>&minus;3.48886e-21</td><td>&minus;3.16921e-21</td><td>&minus;2.81612e-21</td><td>&minus;2.43334e-21</td><td>&minus;2.02489e-21</td><td class=vellipsis>&hellip;</td><td>&minus;1.22676e-21</td><td>&minus;2.59445e-21</td><td>&minus;3.85683e-21</td><td>&minus;4.96471e-21</td><td>&minus;5.87556e-21</td></tr>\n",
       "    <tr><td class='row_index'>249</td><td>&minus;2.44675e-21</td><td>&minus;2.55246e-21</td><td>&minus;2.51868e-21</td><td>&minus;2.33818e-21</td><td>&minus;2.02147e-21</td><td>&minus;1.58701e-21</td><td>&minus;1.06009e-21</td><td>&minus;4.71411e-22</td><td>1.44725e-22</td><td>7.52432e-22</td><td class=vellipsis>&hellip;</td><td>6.94866e-21</td><td>8.64999e-22</td><td>&minus;6.09522e-21</td><td>&minus;6.5838e-21</td><td>&minus;2.73347e-21</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>250 rows &times; 1,577,908 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#2c782ea4f60 250x1577908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dt.fread(r\"\\\\wsl.localhost\\Ubuntu-22.04\\home\\imperious\\ML and CVAE projects\\GW from EMRI\\gw data\\GW_data\\results\\datasignal_one_1.txt\",sep=\"\\t\")\n",
    "df[dt.str32] = dt.float64\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dark_\\AppData\\Local\\Temp\\ipykernel_3088\\569160985.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  params.iloc[:,i:i+1] = params.iloc[:,i:i+1].applymap(lambda x: calcular_formula(str(x)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>mu</th>\n",
       "      <th>a</th>\n",
       "      <th>r0</th>\n",
       "      <th>phi0</th>\n",
       "      <th>thetas</th>\n",
       "      <th>phis</th>\n",
       "      <th>theta1</th>\n",
       "      <th>phi1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.874275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.790708</td>\n",
       "      <td>0.596903</td>\n",
       "      <td>0.125664</td>\n",
       "      <td>3.141593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800000</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.841555</td>\n",
       "      <td>1.944446</td>\n",
       "      <td>1.162389</td>\n",
       "      <td>0.502655</td>\n",
       "      <td>1.319469</td>\n",
       "      <td>2.356194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.235850</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>1.53938</td>\n",
       "      <td>2.70177</td>\n",
       "      <td>1.162389</td>\n",
       "      <td>0.973894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9000000</td>\n",
       "      <td>86</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.286453</td>\n",
       "      <td>2.040121</td>\n",
       "      <td>2.356194</td>\n",
       "      <td>1.507964</td>\n",
       "      <td>2.261947</td>\n",
       "      <td>3.110177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.832572</td>\n",
       "      <td>-0.747715</td>\n",
       "      <td>2.293363</td>\n",
       "      <td>1.476549</td>\n",
       "      <td>1.193805</td>\n",
       "      <td>2.953097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>4000000</td>\n",
       "      <td>66</td>\n",
       "      <td>0.424</td>\n",
       "      <td>7.222384</td>\n",
       "      <td>2.995444</td>\n",
       "      <td>2.890265</td>\n",
       "      <td>2.953097</td>\n",
       "      <td>0.973894</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2000000</td>\n",
       "      <td>89</td>\n",
       "      <td>0.424</td>\n",
       "      <td>9.870412</td>\n",
       "      <td>0.110881</td>\n",
       "      <td>1.53938</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>0.345575</td>\n",
       "      <td>0.219911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>900000</td>\n",
       "      <td>89</td>\n",
       "      <td>0.424</td>\n",
       "      <td>14.051228</td>\n",
       "      <td>2.472069</td>\n",
       "      <td>2.513274</td>\n",
       "      <td>2.70177</td>\n",
       "      <td>1.507964</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>4000000</td>\n",
       "      <td>65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>7.204781</td>\n",
       "      <td>2.342268</td>\n",
       "      <td>2.136283</td>\n",
       "      <td>2.54469</td>\n",
       "      <td>2.858849</td>\n",
       "      <td>2.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>700000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.424</td>\n",
       "      <td>13.121000</td>\n",
       "      <td>1.944446</td>\n",
       "      <td>1.047198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           M  mu      a         r0      phi0    thetas      phis    theta1  \\\n",
       "0     700000  40  0.000  13.874275  0.000000  1.790708  0.596903  0.125664   \n",
       "1     800000  72  0.000  14.841555  1.944446  1.162389  0.502655  1.319469   \n",
       "2    2000000  34  0.000   9.235850  0.003569   1.53938   2.70177  1.162389   \n",
       "3    9000000  86  0.000   7.286453  2.040121  2.356194  1.507964  2.261947   \n",
       "4    9000000  34  0.000   6.832572 -0.747715  2.293363  1.476549  1.193805   \n",
       "..       ...  ..    ...        ...       ...       ...       ...       ...   \n",
       "245  4000000  66  0.424   7.222384  2.995444  2.890265  2.953097  0.973894   \n",
       "246  2000000  89  0.424   9.870412  0.110881   1.53938  0.031416  0.345575   \n",
       "247   900000  89  0.424  14.051228  2.472069  2.513274   2.70177  1.507964   \n",
       "248  4000000  65  0.424   7.204781  2.342268  2.136283   2.54469  2.858849   \n",
       "249   700000  40  0.424  13.121000  1.944446  1.047198       0.0  0.392699   \n",
       "\n",
       "         phi1  \n",
       "0    3.141593  \n",
       "1    2.356194  \n",
       "2    0.973894  \n",
       "3    3.110177  \n",
       "4    2.953097  \n",
       "..        ...  \n",
       "245  2.764602  \n",
       "246  0.219911  \n",
       "247  2.764602  \n",
       "248  2.764602  \n",
       "249       0.0  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dt.fread(r\"\\\\wsl.localhost\\Ubuntu-22.04\\home\\imperious\\ML and CVAE projects\\GW from EMRI\\gw data\\readme\\paramlist.txt\",sep=\"\\t\")\n",
    "params=params[:250,:]\n",
    "params = params.to_pandas()\n",
    "\n",
    "def calcular_formula(formula):\n",
    "  formula = formula.replace(\"Pi\", str(math.pi))\n",
    "  return eval(formula)\n",
    "\n",
    "for i in range(5,9):\n",
    "  params.iloc[:,i:i+1] = params.iloc[:,i:i+1].applymap(lambda x: calcular_formula(str(x)))\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_X = MinMaxScaler()\n",
    "normalizer_X.fit(params) #Fitted with continuous_x. Use it to decode MinMax.\n",
    "\n",
    "norm_params = normalizer_X.transform(params)\n",
    "\n",
    "#print(pd.DataFrame(norm_Params))\n",
    "\n",
    "#normalizing gw data\n",
    "normalizer_cond = MinMaxScaler()\n",
    "normalizer_cond.fit(df)\n",
    "\n",
    "df = normalizer_cond.transform(df) #does sending to the df itself save ram?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2\n",
    "batch_size = 32  #Why does 32 work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_and_batchnorm(x):\n",
    "    return Dropout(0.3)(BatchNormalization()(x))\n",
    "\n",
    "def noiser(args):\n",
    "    global mean, log_var\n",
    "    mean, log_var = args\n",
    "    N = K.random_normal(shape=(K.shape(mean)[0],hidden_dim), mean=0., stddev=1.0)\n",
    "    return K.exp(log_var / 2) * N + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_params = Input(shape=(9,))\n",
    "cond_gw = Input(shape=(1577908,))\n",
    "\n",
    "x = concatenate([input_params, cond_gw])\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = dropout_and_batchnorm(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = dropout_and_batchnorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space\n",
    "mean = Dense(hidden_dim)(x)\n",
    "log_var = Dense(hidden_dim)(x)\n",
    "h = Lambda(noiser, output_shape=(hidden_dim), name=\"latent_space\")([mean, log_var]) # MAIN ERROR. FIX THE SHAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "input_decoder = Input(shape=(hidden_dim,))\n",
    "cond_gw_decoder = Input(shape=(1577908,))\n",
    "d = concatenate([input_decoder,cond_gw_decoder])\n",
    "d = Dense(128, activation=\"relu\")(d)\n",
    "d = dropout_and_batchnorm(d)\n",
    "d = Dense(256, activation=\"relu\")(d)\n",
    "d = dropout_and_batchnorm(d)\n",
    "decoded = Dense(9, activation=\"linear\")(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, y):\n",
    "    loss = K.sum(K.square(x-y), axis=-1)\n",
    "    kl_loss = -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis=-1)\n",
    "    return loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 1577908)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 2)            403981956   ['input_15[0][0]',               \n",
      "                                                                  'input_16[0][0]']               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 1577908)]    0           []                               \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 9)            202009481   ['encoder[0][0]',                \n",
      "                                                                  'input_18[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 605,991,437\n",
      "Trainable params: 605,989,901\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.Model([input_params,cond_gw], h, name=\"encoder\") # Parece estar certo\n",
    "decoder = keras.Model([input_decoder, cond_gw_decoder], decoded, name=\"decoder\") \n",
    "\n",
    "cvae = keras.Model(inputs = [input_params, cond_gw, cond_gw_decoder], #its ok\n",
    "                   outputs = decoder([encoder([input_params, cond_gw]), cond_gw_decoder]),\n",
    "                   name=\"cvae\")\n",
    "\n",
    "cvae.compile(optimizer=\"adam\", loss=vae_loss)\n",
    "\n",
    "plot_model(encoder, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8/8 [==============================] - 67s 5s/step - loss: nan\n",
      "Epoch 2/25\n",
      "8/8 [==============================] - 46s 6s/step - loss: nan\n",
      "Epoch 3/25\n",
      "8/8 [==============================] - 39s 5s/step - loss: nan\n",
      "Epoch 4/25\n",
      "8/8 [==============================] - 39s 5s/step - loss: nan\n",
      "Epoch 5/25\n",
      "3/8 [==========>...................] - ETA: 22s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m      3\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnorm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\dark_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "cvae.fit(\n",
    "    [norm_params, df, df], norm_params,\n",
    "    epochs=epochs,\n",
    "    shuffle=True\n",
    ") \n",
    "\n",
    "# Why is the loss NaN????   x.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
